{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/connor735/344_test/blob/main/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "721baf03",
      "metadata": {
        "id": "721baf03"
      },
      "source": [
        "# Problem 1: nearest neighbors for data imputation\n",
        "\n",
        "Real-world datasets often have missing values: this can be due to hardware faults (e.g., a sensor failing), data corruption, ambiguity, or even by design (think about surveys with optional questions). In Python, the special value `NaN` (not-a-number) can be used to indicate a missing value.\n",
        "\n",
        "Dealing with missing data is not straightforward. One option is to *remove* any sample with missing features $x_{i}$ (or label $y$). However, there are several datasets for which *almost all* samples have one or more missing feature values, and we would be throwing away all the information contained in the non-missing values.\n",
        "\n",
        "In this problem, we explore the use of the $k$-nearest neighbors algorithm for *data imputation*.\n",
        "\n",
        "## Question 1\n",
        "\n",
        "Suppose that you are solving a regression problem where **the only potential missing values are response labels $y^{(i)}$**. Describe a method, based on the $k$-nearest neighbors algorithm, to fill-in missing values based on the available response labels. When do you expect your method to work well, and when do you expect it to produce poor results? Justify your answers and design choices.\n",
        "\n",
        "## Question 2\n",
        "Implement your method from Question 1 by completing the provided code snippet in `p1.py` (in the `KnnLabelImputer` class) and **submit it to Gradescope**. You can use the `regression_dataset_with_missing_responses` function, provided in the next code cell, to generate example inputs to help with development.\n",
        "\n",
        "## Question 3\n",
        "Now, suppose that we are solving a *classification* problem, and we know that *no labels $y^{(i)}$ are missing*. However, there are now missing *feature* values. Like in **Question 1**, describe a method based on the $k$-nearest neighbors algorithm for performing data imputation on the features of the dataset. You can assume that there are enough samples for each distinct class label. When do you expect your method to work well, and when not to? Justify your answers and design choices.\n",
        "\n",
        "**Tip**: The nearest neighbors implementation in `scikit-learn` allows you to define your own distance function via the `metric` parameter. You can use this to define a distance function that takes into account missing values and class membership. Think about how you can modify the distance function to favor samples with the same class label.\n",
        "\n",
        "## Question 4\n",
        "Implement your method from Question 3 by completing the provided code template in `p1.py` (in the `KnnFeatureImputer` class) and **submit it to Gradescope**. You can use the `classification_dataset_with_missing_features` function from the following cell to generate example inputs to help with development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea0855a",
      "metadata": {
        "id": "dea0855a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "def regression_dataset_with_missing_responses(\n",
        "    n_samples: int, n_features: int, p_missing: float = 0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a regression dataset with missing responses.\n",
        "\n",
        "    Args:\n",
        "        n_samples: Number of samples.\n",
        "        n_features: Number of features.\n",
        "        p_missing: Probability of missing a response.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame with features and a response column.\n",
        "    \"\"\"\n",
        "    X, y = make_regression(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "    )\n",
        "    # Replace response values by NaN with probability p_missing.\n",
        "    y = np.where(np.random.rand(*y.shape) < p_missing, np.nan, y)\n",
        "    X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
        "    return pd.concat([X, pd.Series(y, name=\"response\")], axis=1)\n",
        "\n",
        "\n",
        "def classification_dataset_with_missing_features(\n",
        "    n_samples: int, n_features: int, p_missing: float = 0.1, n_classes: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a classification dataset with missing features.\n",
        "\n",
        "    Args:\n",
        "        n_samples: Number of samples.\n",
        "        n_features: Number of features.\n",
        "        p_missing: Probability of missing a feature.\n",
        "        n_classes: Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame with features and a label column.\n",
        "    \"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_classes=n_classes,\n",
        "        n_clusters_per_class=1,\n",
        "    )\n",
        "    # Replace feature values in X by NaN with probability p_missing.\n",
        "    X = np.where(np.random.rand(*X.shape) < p_missing, np.nan, X)\n",
        "    X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
        "    return pd.concat([X, pd.Series(y, name=\"label\")], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb9b43",
      "metadata": {
        "id": "7abb9b43"
      },
      "outputs": [],
      "source": [
        "# Example: regression dataset with 500 samples 20 features, and approximately 10% of responses missing\n",
        "dataset = regression_dataset_with_missing_responses(500, 10, p_missing=0.1)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284366d8",
      "metadata": {
        "id": "284366d8"
      },
      "outputs": [],
      "source": [
        "# Example: classification dataset with 500 samples 10 features, and 10% of responses missing\n",
        "dataset = classification_dataset_with_missing_features(500, 10, p_missing=0.1)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e6b4b0",
      "metadata": {
        "id": "e9e6b4b0"
      },
      "source": [
        "# Problem 2: cross-validation for imbalanced datasets\n",
        "\n",
        "In class, we discussed the use of cross-validation to estimate model performance on unseen data. However, extra care must be taken in cases where our data is *unbalanced*. This is best explained via a cautionary (fictional) tale.\n",
        "\n",
        "\n",
        "> An aerospace engineer is tasked with creating and fine-tuning a classifier that processes post-flight images of aircraft wings and screens them for the presence of fractures. To train the classifier, they are given access to a dataset of labeled images ($0$ indicates no fracture, $1$ indicates fracture). Following the advice of an LLM, the engineer writes a `scikit-learn` pipeline that:\n",
        ">\n",
        "> - pre-processes the data, splits them into $K = 20$ folds using the built-in `KFold` class\n",
        "> - trains several logistic regression models, one for each fold, and\n",
        "> - selects the best model based on the estimated (via cross-validation) F1 score.\n",
        ">\n",
        "> However, 5 years after they deploy their model, maintenance engineers suggest that it failed to detect several fractures, leading to higher degradation of the aircraft and many more dollars lost. **What went wrong?**\n",
        "\n",
        "\n",
        "A likely answer is that the dataset is very imbalanced. Specifically, wings are usually structurally intact after flight; unless special care was taken while curating the dataset, most labels will be negative and many folds will not contain any samples from the positive class. Therefore, it is possible that a classifier that always outputs \"negative\" leads to good F1 score as estimated via cross-validation.\n",
        "\n",
        "To fix this, we augment K-fold cross validation with the idea of **stratified** sampling. Stratified sampling splits the data in a way that approximately preserves the fraction of class labels within each fold. In `scikit-learn`, this is easy to implement via the `StratifiedKFold` class.\n",
        "\n",
        "## Question 1\n",
        "\n",
        "Use the provided function `make_imbalanced_classification_dataset()` below to generate datasets with 500 samples and 50 features. Train a logistic regression classifier using (a) `KFold` and (b) `StratifiedKFold` cross-validation for different values of $K$, and plot the resulting F1 score as a function of the number of folds $K$. Are the results expected or surprising, and why?\n",
        "\n",
        "> **Note**: you can implement the cross-validation yourself or use the built-in `cross_val_score` function from `scikit-learn` or use the `LogisticRegressionCV` model with the appropriate cross-validation generator.\n",
        "\n",
        "## Question 2\n",
        "Another potential approach is to adjust the classification threshold (instead of the default $\\frac{1}{2}$). For example, we can make the classifier much more conservative by outputting a positive label after a threshold of $\\frac{1}{10}$. Do you think this is a good idea? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b114452",
      "metadata": {
        "id": "2b114452"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def make_imbalanced_classification_dataset(\n",
        "    n_samples: int, n_features: int, class_imbalance: float\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Make an imbalanced classification dataset.\n",
        "\n",
        "    Args:\n",
        "        n_samples: The number of samples.\n",
        "        n_features: The number of features.\n",
        "        class_imbalance: The proportion of samples assigned to each class.\n",
        "    \"\"\"\n",
        "    if class_imbalance <= 0 or class_imbalance >= 1:\n",
        "        raise ValueError(\"class_imbalance must be between 0 and 1\")\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features,\n",
        "        n_classes=2,\n",
        "        weights=[class_imbalance, 1 - class_imbalance],\n",
        "    )\n",
        "    X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
        "    return pd.concat([X, pd.Series(y, name=\"label\")], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d71392e",
      "metadata": {
        "id": "8d71392e"
      },
      "outputs": [],
      "source": [
        "dataset = make_imbalanced_classification_dataset(100, 10, 0.1)\n",
        "dataset.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82cf421a",
      "metadata": {
        "id": "82cf421a"
      },
      "source": [
        "# Problem 3: dimension reduction via random projections\n",
        "\n",
        "In class, we discussed how PCA discovers a few important degrees of freedom of high-dimensional datasets.\n",
        "\n",
        "The main idea was the following:\n",
        "\n",
        "> *If data are mostly contained on a low-dimensional subspace, let's discover that subspace and project the data onto it.*\n",
        "\n",
        "The number of principal components, $k$ reflects our \"guess\" for the dimension of that subspace.\n",
        "\n",
        "Another dimension reduction method featuring prominently in theoretical computer science is that of so-called \"random projections\". Setting the stage, suppose we have $n$ data points living in $d$-dimensional space ($\\mathbb{R}^d$ for simplicity). The random projections method proceeds as follows:\n",
        "\n",
        "- Generate $k$ vectors, $g^{(1)}, \\dots, g^{(k)}$, randomly (often according to a Gaussian distribution).\n",
        "- Arrange these vectors as the columns of a matrix:\n",
        "\n",
        "$$\n",
        "\\Pi := \\begin{bmatrix}\n",
        "\tg^{(1)} & \\cdots & g^{(k)}\n",
        "\\end{bmatrix} \\in \\mathbb{R}^{d \\times k}.\n",
        "$$\n",
        "\n",
        "- For each of the $n$ points in your dataset, say $x^{(1)}, \\dots, x^{(n)}$:\n",
        "\t- Compute the product $\\widetilde{x}^{(i)} = \\Pi^{\\top} x^{(i)}$.\n",
        "- This results in a matrix $\\widetilde{X} \\in \\mathbb{R}^{n \\times k}$ containing the \"reduced\" dataset.\n",
        "- Downstream tasks (e.g., regression or classification) can be performed on the reduced dataset instead.\n",
        "    - Any new data point $x$ can be projected to $\\widetilde{x} := \\Pi^{\\top} x$.\n",
        "\n",
        "\n",
        "The random projections method works because the matrix $\\Pi$ produces \"low distortion\": for all pairs $i \\neq j$,\n",
        "\n",
        "$$\n",
        "(1 - \\varepsilon) \\|x^{(i)} - x^{(j)}\\| \\leq \\| \\widetilde{x}^{(i)} - \\widetilde{x}^{(j)}\\| \\leq (1 + \\varepsilon) \\|x^{(i)} - x^{(j)}\\|,\n",
        "$$\n",
        "\n",
        "whenever the number of the random vectors $k$ satisfies the following inequality:\n",
        "\n",
        "$$\n",
        "k \\geq \\frac{10 \\log(n)}{\\varepsilon^2}.\n",
        "$$\n",
        "\n",
        "> Note that the ambient dimension, $d$, **does not appear** in the bound at all! This result is known in the literature as the *Johnson-Lindenstrauss lemma*.\n",
        "\n",
        "In this exercise, you will explore the use of random projections with 2 different variants: one where the random vectors are dense, and one where the random vectors are *sparse* (i.e., mostly zeros). Then, you will compare the quality of the resulting embeddings with that provided by principal component analysis.\n",
        "\n",
        "## Question 1\n",
        "\n",
        "Download the code template `p3.py`. You will find two functions with the following code signatures:\n",
        "\n",
        "```python\n",
        "def apply_sparse_random_projections(dataset: ArrayLike, num_projections: int) -> ArrayLike:\n",
        "\t...\n",
        "\n",
        "def apply_dense_random_projections(dataset: ArrayLike, num_projections: int) -> ArrayLike:\n",
        "\t...\n",
        "```\n",
        "\n",
        "These functions accept a `dataset` (e.g., a Numpy table) where each row is a sample and the number of projections $k$, `num_projections`. They should return the reduced dataset after applying the random projections method. Implement both methods and submit your code to Gradescope.\n",
        "\n",
        "> **Tip**: Use the `SparseRandomProjection` and `GaussianRandomProjection` classes from `scikit-learn`. You will have to instantiate them with the appropriate arguments.\n",
        "\n",
        "## Question 2\n",
        "\n",
        "Use the provided snippet below to create a high-dimensional dataset. Using your implementation from Question 1, compute the following quantities for $k = 2^{j}$ ($j = 2, \\dots, 10$):\n",
        "\n",
        "- **Maximum and minimum distortion**: the min and max ratio of distances between projected and original data points.\n",
        "\n",
        "  $$\n",
        "  \\gamma_{\\mathsf{upper}} := \\max_{i \\neq j} d_{i, j} \\;\\; \\text{and} \\;\\; \\gamma_{\\mathsf{lower}} := \\min_{i \\neq j} d_{i, j},\n",
        "  $$\n",
        "  \n",
        "  where we define the ratios $d_{i,j}$ as follows:\n",
        "  \n",
        "  $$\n",
        "  \\frac{\\|\\widetilde{x}^{(i)} - \\widetilde{x}^{(j)}\\|}{\\|x^{(i)} - x^{(j)}\\|}.\n",
        "  $$\n",
        "\n",
        "- The ratio $\\sqrt{\\frac{\\lceil \\log n \\rceil}{k}}$.\n",
        "\n",
        "Make a plot of these quantities as a function of $k$ (once for sparse and once for dense random projections). In your narrative writeup, discuss the following questions:\n",
        "\n",
        "- Is the behavior of these quantities surprising as you increase $k$? Why or why not?\n",
        "- Is the behavior of these quantities surprising as you move from sparse to dense random projections? Why or why not?\n",
        "- Would you recommend this method for a huge dataset (very large $n$) of relatively small dimension ($d$)? Why or why not?\n",
        "\n",
        "## Question 3\n",
        "\n",
        "For the same dataset, compare the distortion factors from Question 1 with those attained by PCA (using $k$ as the number of  principal components); do this **for all** $k$ you tried in Question 1 and make a shared plot depicting the distortion factors for both methods as a function of $k$.\n",
        "\n",
        "- Are the distortion factors from PCA higher or lower?\n",
        "- Which distortion factor did you expect to be higher before you completed the question, and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346bd378",
      "metadata": {
        "id": "346bd378"
      },
      "outputs": [],
      "source": [
        "# Code to create a high-dimensional dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_friedman1\n",
        "\n",
        "\n",
        "def create_high_dim_dataset(n_samples: int, n_features: int) -> pd.DataFrame:\n",
        "    X, y = make_friedman1(\n",
        "        n_samples=n_samples,\n",
        "        n_features=n_features - 1,\n",
        "        noise=0.1,\n",
        "        random_state=42,\n",
        "    )\n",
        "    return pd.DataFrame(np.hstack([X, y.reshape(-1, 1)]))\n",
        "\n",
        "\n",
        "# Example: dataset with 128 samples, 2048 features, and some noise.\n",
        "X = create_high_dim_dataset(128, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e4dcc3",
      "metadata": {
        "id": "43e4dcc3"
      },
      "source": [
        "# Problem 4: Kaggle competition\n",
        "\n",
        "In this problem, you will work on the [Diabetes Health Indicators dataset](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset) from Kaggle. The dataset comprises responses to a CDC survey. The code in the cell below splits the dataset into features and labels, resulting in a binary classification task. For this problem, your only deliverable should be a Jupyter notebook that implements all the necessary code and addresses the following questions:\n",
        "\n",
        "## Question 1\n",
        "\n",
        "The dataset contains numeric, nominal and ordinal features. Which features fall into which category, and how should we preprocess each category?\n",
        "\n",
        "## Question 2\n",
        "\n",
        "Train a classifier for the binary classification task, and report the average accuracy and Area-Under-Curve (AUC) using 5-fold cross-validation. Does this problem warrant using additional metrics, such as the F1 score, in addition to accuracy? Why or why not?\n",
        "\n",
        "**Note**: You are allowed to choose any model suitable for binary classification you want, in addition to those you have already seen in class. You are welcome to peruse the scikit-learn documentation for additional models, if you want to try them out.\n",
        "\n",
        "## Question 3\n",
        "\n",
        "If you wanted to further improve the performance of your classifier for this problem, what additional methods / model improvements would you consider and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd76e157",
      "metadata": {
        "id": "fd76e157"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Download latest version\n",
        "path = os.path.join(\n",
        "    kagglehub.dataset_download(\"mohankrishnathalla/diabetes-health-indicators-dataset\"),\n",
        "    \"diabetes_dataset.csv\",\n",
        ")\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Define the feature and labels\n",
        "# We want to solve the binary classification task, drop the other two labels\n",
        "features = df.drop(columns=[\"diabetes_risk_score\", \"diabetes_stage\", \"diagnosed_diabetes\"])\n",
        "labels = df[\"diagnosed_diabetes\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91534985",
      "metadata": {
        "id": "91534985"
      },
      "outputs": [],
      "source": [
        "features.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179e3c86",
      "metadata": {
        "id": "179e3c86"
      },
      "outputs": [],
      "source": [
        "labels.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}